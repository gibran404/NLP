{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# txt to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from docx import Document\n",
    "# from docx.shared import RGBColor\n",
    "# import re\n",
    "\n",
    "# # Function to map labels to colors\n",
    "# def get_color(label):\n",
    "#     colors = {\n",
    "#         'A': RGBColor(255, 0, 0),    # Red\n",
    "#         'B': RGBColor(0, 255, 0),    # Green\n",
    "#         'C': RGBColor(0, 0, 255),    # Blue\n",
    "#         # Add more labels and corresponding colors as needed\n",
    "#     }\n",
    "#     return colors.get(label, RGBColor(0, 0, 0))  # Default to black if label not found\n",
    "\n",
    "# # Read the dataframe\n",
    "# df = pd.read_csv('labels.csv')\n",
    "\n",
    "# # Create a dictionary from the dataframe\n",
    "# word_label_dict = dict(zip(df['word'], df['label']))\n",
    "\n",
    "# # Read the text file\n",
    "# with open('input_text.txt', 'r') as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# # Create a new Word document\n",
    "# doc = Document()\n",
    "# paragraph = doc.add_paragraph()\n",
    "\n",
    "# # Split text into words while keeping the delimiters\n",
    "# words = re.split('(\\W+)', text)\n",
    "\n",
    "# # Add words to the paragraph with respective colors\n",
    "# for word in words:\n",
    "#     if word in word_label_dict:\n",
    "#         run = paragraph.add_run(word)\n",
    "#         run.font.color.rgb = get_color(word_label_dict[word])\n",
    "#     else:\n",
    "#         paragraph.add_run(word)\n",
    "\n",
    "# # Save the document\n",
    "# doc.save('colored_text.docx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import fitz  # PyMuPDF\n",
    "# import re\n",
    "\n",
    "# # Function to map labels to colors\n",
    "# def get_color(label):\n",
    "#     colors = {\n",
    "#         'A': (1, 0, 0),    # Red\n",
    "#         'B': (0, 1, 0),    # Green\n",
    "#         'C': (0, 0, 1),    # Blue\n",
    "#         # Add more labels and corresponding colors as needed\n",
    "#     }\n",
    "#     return colors.get(label, (0, 0, 0))  # Default to black if label not found\n",
    "\n",
    "# # Read the dataframe\n",
    "# df = pd.read_csv('labels.csv')\n",
    "\n",
    "# # Create a dictionary from the dataframe\n",
    "# word_label_dict = dict(zip(df['word'], df['label']))\n",
    "\n",
    "# # Read the PDF file\n",
    "# pdf_document = fitz.open(\"input_text.pdf\")\n",
    "# output_document = fitz.open()  # New PDF for the output\n",
    "\n",
    "# for page_num in range(len(pdf_document)):\n",
    "#     page = pdf_document[page_num]\n",
    "#     text = page.get_text(\"text\")  # Extract text from the page\n",
    "#     output_page = output_document.new_page(width=page.rect.width, height=page.rect.height)\n",
    "#     text_words = re.split('(\\W+)', text)\n",
    "\n",
    "#     # To track the position of text\n",
    "#     current_x, current_y = 0, 0\n",
    "#     font_size = 12  # Default font size, you might need to adjust this based on your PDF\n",
    "\n",
    "#     for word in text_words:\n",
    "#         if word.strip():  # Only process non-empty words\n",
    "#             color = get_color(word_label_dict.get(word, None))\n",
    "#             output_page.insert_text(\n",
    "#                 (current_x, current_y),  # Position where to insert text\n",
    "#                 word,\n",
    "#                 fontname=\"helv\",\n",
    "#                 fontsize=font_size,\n",
    "#                 color=color\n",
    "#             )\n",
    "#             current_x += fitz.get_text_length(word, fontname=\"helv\", fontsize=font_size)\n",
    "\n",
    "#         if '\\n' in word:\n",
    "#             current_x = 0\n",
    "#             current_y += font_size\n",
    "\n",
    "# # Save the output PDF\n",
    "# output_document.save(\"colored_text.pdf\")\n",
    "\n",
    "# # Close the documents\n",
    "# pdf_document.close()\n",
    "# output_document.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf to word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# from docx import Document\n",
    "# from docx.shared import RGBColor\n",
    "\n",
    "# # Function to map labels to colors\n",
    "# def get_color(label):\n",
    "#     colors = {\n",
    "#         'A': RGBColor(255, 0, 0),    # Red\n",
    "#         'B': RGBColor(0, 255, 0),    # Green\n",
    "#         'C': RGBColor(0, 0, 255),    # Blue\n",
    "#         # Add more labels and corresponding colors as needed\n",
    "#     }\n",
    "#     return colors.get(label, RGBColor(0, 0, 0))  # Default to black if label not found\n",
    "\n",
    "# # Path to the PDF file\n",
    "# pdf_path = 'input_text.pdf'\n",
    "\n",
    "# # Path to the CSV file containing word labels\n",
    "# csv_path = 'labels.csv'\n",
    "\n",
    "# # Read the CSV file\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# # Create a dictionary from the dataframe\n",
    "# word_label_dict = dict(zip(df['word'], df['label']))\n",
    "\n",
    "# # Open the PDF file\n",
    "# pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "# # Create a new Word document\n",
    "# doc = Document()\n",
    "\n",
    "# # Iterate over each page in the PDF\n",
    "# for page_num in range(len(pdf_document)):\n",
    "#     page = pdf_document.load_page(page_num)\n",
    "#     text = page.get_text()\n",
    "\n",
    "#     # Split text into paragraphs\n",
    "#     paragraphs = text.split('\\n')\n",
    "\n",
    "#     # Add paragraphs to the Word document with respective colors\n",
    "#     for paragraph_text in paragraphs:\n",
    "#         paragraph = doc.add_paragraph()\n",
    "\n",
    "#         # Split paragraph into words\n",
    "#         words = paragraph_text.split()\n",
    "\n",
    "#         # Add words to the paragraph with respective colors\n",
    "#         for word in words:\n",
    "#             if word in word_label_dict:\n",
    "#                 run = paragraph.add_run(word + ' ')\n",
    "#                 run.font.color.rgb = get_color(word_label_dict[word])\n",
    "#             else:\n",
    "#                 paragraph.add_run(word + ' ')\n",
    "\n",
    "# # Save the document\n",
    "# doc.save('colored_text.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_ne():\n",
    "#     pdf_path = 'input_text.pdf'\n",
    "\n",
    "#     # Path to the CSV file containing word labels\n",
    "#     csv_path = 'labels.csv'\n",
    "\n",
    "#     # Read the CSV file\n",
    "#     df = pd.read_csv(csv_path)\n",
    "\n",
    "#     # Create a dictionary from the dataframe\n",
    "#     word_label_dict = dict(zip(df['word'], df['label']))\n",
    "\n",
    "#     # Open the PDF file\n",
    "#     pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "#     # Create a new Word document\n",
    "#     doc = Document()\n",
    "\n",
    "#     # Iterate over each page in the PDF\n",
    "#     for page_num in range(len(pdf_document)):\n",
    "#         page = pdf_document.load_page(page_num)\n",
    "#         text = page.get_text()\n",
    "\n",
    "#         # Split text into paragraphs\n",
    "#         paragraphs = text.split('\\n')\n",
    "\n",
    "#         # Add paragraphs to the Word document with respective colors\n",
    "#         for paragraph_text in paragraphs:\n",
    "#             paragraph = doc.add_paragraph()\n",
    "\n",
    "#             # Split paragraph into words\n",
    "#             words = paragraph_text.split()\n",
    "\n",
    "#             # Add words to the paragraph with respective colors\n",
    "#             for word in words:\n",
    "#                 if word in word_label_dict:\n",
    "#                     run = paragraph.add_run(word + ' ')\n",
    "#                     run.font.color.rgb = get_color(word_label_dict[word])\n",
    "#                 else:\n",
    "#                     paragraph.add_run(word + ' ')\n",
    "\n",
    "#     # Save the document\n",
    "#     doc.save('colored_text.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hanan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "# Load SpaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ur = spacy.load(\"xx_ent_wiki_sm\")  # Multilingual model, as there's no dedicated Urdu model\n",
    "translator = Translator()\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    return translator.translate(text, src='ur', dest='en').text\n",
    "\n",
    "def translate_to_urdu(text):\n",
    "    return translator.translate(text, src='en', dest='ur').text\n",
    "\n",
    "def apply_ner(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_ur(text)\n",
    "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def highlight_entities(text, entities):\n",
    "    highlighted_text = text\n",
    "    for entity in entities:\n",
    "        highlighted_text = highlighted_text.replace(entity['text'], colored(entity['text'], 'blue', attrs=['bold']))\n",
    "    return highlighted_text\n",
    "\n",
    "\n",
    "\n",
    "def get_entities(text):\n",
    "    language = detect_language(text)\n",
    "    original_text = text\n",
    "\n",
    "    if language == 'ur':\n",
    "        text = translate_to_english(text)\n",
    "\n",
    "    entities = apply_ner(text, 'en')\n",
    "    \n",
    "    print (entities)\n",
    "    \n",
    "    if language == 'ur':\n",
    "        translated_entities = []\n",
    "        for entity in entities:\n",
    "            translated_text = translate_to_urdu(entity['text'])\n",
    "            translated_entities.append({'text': translated_text, 'label': entity['label']})\n",
    "        entities = translated_entities\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "def highlight_doc(doc, entities_list):\n",
    "    # in the doc, depending on the entities class, highlight the text in that color. each class should have a unique color\n",
    "    \n",
    "    colors = {\n",
    "        'PERSON': 'red',\n",
    "        'NORP': 'green',\n",
    "        'FAC': 'blue',\n",
    "        'ORG': 'yellow',\n",
    "        'GPE': 'purple',\n",
    "        'LOC': 'orange',\n",
    "        'PRODUCT': 'pink',\n",
    "        'EVENT': 'brown',\n",
    "        'WORK_OF_ART': 'grey',\n",
    "        'LAW': 'black',\n",
    "        'LANGUAGE': 'white',\n",
    "        'DATE': 'cyan',\n",
    "        'TIME': 'magenta',\n",
    "        'PERCENT': 'lightgrey',\n",
    "        'MONEY': 'darkgrey',\n",
    "        'QUANTITY': 'lightblue',\n",
    "        'ORDINAL': 'lightgreen',\n",
    "        'CARDINAL': 'lightyellow'\n",
    "    }\n",
    "    \n",
    "    # iterate over the entities and highlight the text in the doc\n",
    "    for page_num in range(len(doc)):\n",
    "        page = Document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        # Split text into paragraphs\n",
    "        paragraphs = text.split('\\n')\n",
    "\n",
    "        # Add paragraphs to the Word document with respective colors\n",
    "        for paragraph_text in paragraphs:\n",
    "            paragraph = doc.add_paragraph()\n",
    "\n",
    "            # Split paragraph into words\n",
    "            words = paragraph_text.split()\n",
    "\n",
    "            # Add words to the paragraph with respective colors\n",
    "            for word in words:\n",
    "                if word in entities_list:\n",
    "                    run = paragraph.add_run(word + ' ')\n",
    "                    run.font.color.rgb = RGBColor(colors[entities_list[word]], 0, 0)\n",
    "                else:\n",
    "                    paragraph.add_run(word + ' ')\n",
    "    \n",
    "def display_ner_visualization(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_ur(text)\n",
    "    displacy.render(doc, style='ent', jupyter=False)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non-printable and non-XML compatible characters\n",
    "    return ''.join(c for c in text if c.isprintable() and c not in '\\x00-\\x1f\\x7f-\\x9f')\n",
    "\n",
    "def extract_ne(pdf_path, output_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    doc = Document()\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        entities_list = get_entities(text)\n",
    "        \n",
    "        # highlight_doc(pdf_document, entities_list)\n",
    "\n",
    "        # Split text into paragraphs\n",
    "        paragraphs = text.split('\\n')\n",
    "\n",
    "        # Add paragraphs to the Word document with respective colors\n",
    "        for paragraph_text in paragraphs:\n",
    "            paragraph = doc.add_paragraph()\n",
    "\n",
    "            # Split paragraph into words\n",
    "            words = paragraph_text.split()\n",
    "\n",
    "            # Add words to the paragraph with respective colors\n",
    "            for word in words:\n",
    "                if word in entities_list:\n",
    "                    run = paragraph.add_run(word + ' ')\n",
    "                    run.font.color.rgb = get_color(entities_list[word])\n",
    "                else:\n",
    "                    paragraph.add_run(word + ' ')\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(\"Named entities extracted and saved to\", output_path)\n",
    "\n",
    "def get_color(label):\n",
    "    # return rgb value for the color\n",
    "    if label == 'PERSON':\n",
    "        return RGBColor(255, 0, 0)\n",
    "    elif label == 'NORP':\n",
    "        return RGBColor(0, 255, 0)\n",
    "    elif label == 'FAC':\n",
    "        return RGBColor(0, 0, 255)\n",
    "    elif label == 'ORG':\n",
    "        return RGBColor(255, 255, 0)\n",
    "    elif label == 'GPE':\n",
    "        return RGBColor(255, 0, 255)\n",
    "    elif label == 'LOC':\n",
    "        return RGBColor(0, 255, 255)\n",
    "    elif label == 'PRODUCT':\n",
    "        return RGBColor(255, 255, 255)\n",
    "    elif label == 'EVENT':\n",
    "        return RGBColor(128, 0, 0)\n",
    "    elif label == 'WORK_OF_ART':\n",
    "        return RGBColor(0, 128, 0)\n",
    "    elif label == 'LAW':\n",
    "        return RGBColor(0, 0, 128)\n",
    "    elif label == 'LANGUAGE':\n",
    "        return RGBColor(128, 128, 0)\n",
    "    elif label == 'DATE':\n",
    "        return RGBColor(128, 0, 128)\n",
    "    elif label == 'TIME':\n",
    "        return RGBColor(0, 128, 128)\n",
    "    elif label == 'PERCENT':\n",
    "        return RGBColor(128, 128, 128)\n",
    "    elif label == 'MONEY':\n",
    "        return RGBColor(192, 192, 192)\n",
    "    elif label == 'QUANTITY':\n",
    "        return RGBColor(128, 192, 192)\n",
    "    elif label == 'ORDINAL':\n",
    "        return RGBColor(192, 128, 192)\n",
    "    elif label == 'CARDINAL':\n",
    "        return RGBColor(192, 192, 128)\n",
    "    else:\n",
    "        return RGBColor(0, 0, 0)\n",
    "    \n",
    "colors = {\n",
    "        'PERSON': 'red',\n",
    "        'NORP': 'green',\n",
    "        'FAC': 'blue',\n",
    "        'ORG': 'yellow',\n",
    "        'GPE': 'purple',\n",
    "        'LOC': 'orange',\n",
    "        'PRODUCT': 'pink',\n",
    "        'EVENT': 'brown',\n",
    "        'WORK_OF_ART': 'grey',\n",
    "        'LAW': 'black',\n",
    "        'LANGUAGE': 'white',\n",
    "        'DATE': 'cyan',\n",
    "        'TIME': 'magenta',\n",
    "        'PERCENT': 'lightgrey',\n",
    "        'MONEY': 'darkgrey',\n",
    "        'QUANTITY': 'lightblue',\n",
    "        'ORDINAL': 'lightgreen',\n",
    "        'CARDINAL': 'lightyellow'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT's\n",
    "\n",
    "works but hyphens etc fuck with the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "from termcolor import colored\n",
    "\n",
    "# Load SpaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ur = spacy.load(\"xx_ent_wiki_sm\")  # Multilingual model, as there's no dedicated Urdu model\n",
    "translator = Translator()\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    return translator.translate(text, src='ur', dest='en').text\n",
    "\n",
    "def translate_to_urdu(text):\n",
    "    return translator.translate(text, src='en', dest='ur').text\n",
    "\n",
    "def apply_ner(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_ur(text)\n",
    "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def get_entities(text):\n",
    "    language = detect_language(text)\n",
    "    original_text = text\n",
    "\n",
    "    if language == 'ur':\n",
    "        text = translate_to_english(text)\n",
    "\n",
    "    entities = apply_ner(text, 'en')\n",
    "\n",
    "    if language == 'ur':\n",
    "        translated_entities = []\n",
    "        for entity in entities:\n",
    "            translated_text = translate_to_urdu(entity['text'])\n",
    "            translated_entities.append({'text': translated_text, 'label': entity['label']})\n",
    "        entities = translated_entities\n",
    "\n",
    "    return entities\n",
    "\n",
    "def get_color(label):\n",
    "    # return rgb value for the color\n",
    "    colors = {\n",
    "        'PERSON': RGBColor(255, 0, 0),\n",
    "        'NORP': RGBColor(0, 255, 0),\n",
    "        'FAC': RGBColor(0, 0, 255),\n",
    "        'ORG': RGBColor(255, 255, 0),\n",
    "        'GPE': RGBColor(255, 0, 255),\n",
    "        'LOC': RGBColor(0, 255, 255),\n",
    "        'PRODUCT': RGBColor(255, 192, 203),\n",
    "        'EVENT': RGBColor(139, 69, 19),\n",
    "        'WORK_OF_ART': RGBColor(169, 169, 169),\n",
    "        'LAW': RGBColor(0, 0, 0),\n",
    "        'LANGUAGE': RGBColor(255, 255, 255),\n",
    "        'DATE': RGBColor(0, 255, 255),\n",
    "        'TIME': RGBColor(255, 0, 255),\n",
    "        'PERCENT': RGBColor(211, 211, 211),\n",
    "        'MONEY': RGBColor(105, 105, 105),\n",
    "        'QUANTITY': RGBColor(173, 216, 230),\n",
    "        'ORDINAL': RGBColor(144, 238, 144),\n",
    "        'CARDINAL': RGBColor(255, 255, 224)\n",
    "    }\n",
    "    return colors.get(label, RGBColor(0, 0, 0))\n",
    "\n",
    "\n",
    "\n",
    "def extract_ne(pdf_path, output_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    doc = Document()\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        entities_list = get_entities(text)\n",
    "\n",
    "        # Split text into paragraphs\n",
    "        paragraphs = text.split('\\n')\n",
    "\n",
    "        # Add paragraphs to the Word document with respective colors\n",
    "        for paragraph_text in paragraphs:\n",
    "            paragraph = doc.add_paragraph()\n",
    "\n",
    "            # Split paragraph into words\n",
    "            words = paragraph_text.split()\n",
    "\n",
    "            # Add words to the paragraph with respective colors\n",
    "            for word in words:\n",
    "                matched_entity = next((entity for entity in entities_list if entity['text'] == word), None)\n",
    "                if matched_entity:\n",
    "                    run = paragraph.add_run(word + ' ')\n",
    "                    run.font.color.rgb = get_color(matched_entity['label'])\n",
    "                else:\n",
    "                    paragraph.add_run(word + ' ')\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(\"Named entities extracted and saved to\", output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load SpaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_ur = spacy.load(\"xx_ent_wiki_sm\")  # Multilingual model, as there's no dedicated Urdu model\n",
    "translator = Translator()\n",
    "\n",
    "def detect_language(text):\n",
    "    return detect(text)\n",
    "\n",
    "def translate_to_english(text):\n",
    "    return translator.translate(text, src='ur', dest='en').text\n",
    "\n",
    "def translate_to_urdu(text):\n",
    "    return translator.translate(text, src='en', dest='ur').text\n",
    "\n",
    "def apply_ner(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    else:\n",
    "        doc = nlp_ur(text)\n",
    "    entities = [{'text': ent.text, 'label': ent.label_} for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def get_entities(text):\n",
    "    language = detect_language(text)\n",
    "    original_text = text\n",
    "\n",
    "    if language == 'ur':\n",
    "        text = translate_to_english(text)\n",
    "\n",
    "    entities = apply_ner(text, 'en')\n",
    "\n",
    "    if language == 'ur':\n",
    "        translated_entities = []\n",
    "        for entity in entities:\n",
    "            translated_text = translate_to_urdu(entity['text'])\n",
    "            translated_entities.append({'text': translated_text, 'label': entity['label']})\n",
    "        entities = translated_entities\n",
    "\n",
    "    return entities\n",
    "\n",
    "def get_color(label):\n",
    "    colors = {\n",
    "        'PERSON': RGBColor(255, 0, 0),\n",
    "        'NORP': RGBColor(0, 255, 0),\n",
    "        'FAC': RGBColor(0, 0, 255),\n",
    "        'ORG': RGBColor(255, 255, 0),\n",
    "        'GPE': RGBColor(255, 0, 255),\n",
    "        'LOC': RGBColor(0, 255, 255),\n",
    "        'PRODUCT': RGBColor(255, 192, 203),\n",
    "        'EVENT': RGBColor(139, 69, 19),\n",
    "        'WORK_OF_ART': RGBColor(169, 169, 169),\n",
    "        'LAW': RGBColor(0, 0, 0),\n",
    "        'LANGUAGE': RGBColor(255, 255, 255),\n",
    "        'DATE': RGBColor(0, 255, 255),\n",
    "        'TIME': RGBColor(255, 0, 255),\n",
    "        'PERCENT': RGBColor(211, 211, 211),\n",
    "        'MONEY': RGBColor(105, 105, 105),\n",
    "        'QUANTITY': RGBColor(173, 216, 230),\n",
    "        'ORDINAL': RGBColor(144, 238, 144),\n",
    "        'CARDINAL': RGBColor(255, 255, 224)\n",
    "    }\n",
    "    return colors.get(label, RGBColor(0, 0, 0))\n",
    "\n",
    "def extract_ne(pdf_path, output_path):\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    doc = Document()\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "\n",
    "        entities_list = get_entities(text)\n",
    "        current_word = ''\n",
    "        \n",
    "        paragraph = doc.add_paragraph()\n",
    "        \n",
    "        for char in text:\n",
    "            if char.isalpha():\n",
    "                current_word += char\n",
    "            else:\n",
    "                if current_word:\n",
    "                    matched_entity = next((entity for entity in entities_list if entity['text'] == current_word), None)\n",
    "                    if matched_entity:\n",
    "                        run = paragraph.add_run(current_word)\n",
    "                        run.font.color.rgb = get_color(matched_entity['label'])\n",
    "                    else:\n",
    "                        paragraph.add_run(current_word)\n",
    "                    current_word = ''\n",
    "                if char.isspace():\n",
    "                    paragraph.add_run(char)\n",
    "                else:\n",
    "                    run = paragraph.add_run(char)\n",
    "                    run.font.color.rgb = RGBColor(0, 0, 0)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(\"Named entities extracted and saved to\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mextract_ne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_text.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolored_text.docx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m, in \u001b[0;36mextract_ne\u001b[1;34m(pdf_path, output_path)\u001b[0m\n\u001b[0;32m     77\u001b[0m page \u001b[38;5;241m=\u001b[39m pdf_document\u001b[38;5;241m.\u001b[39mload_page(page_num)\n\u001b[0;32m     78\u001b[0m text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[1;32m---> 80\u001b[0m entities_list \u001b[38;5;241m=\u001b[39m \u001b[43mget_entities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m current_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m paragraph \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39madd_paragraph()\n",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m, in \u001b[0;36mget_entities\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     33\u001b[0m original_text \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mur\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_to_english\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m entities \u001b[38;5;241m=\u001b[39m apply_ner(text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mur\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mtranslate_to_english\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_to_english\u001b[39m(text):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mur\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Users\\gibra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:182\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    181\u001b[0m origin \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m--> 182\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\gibra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[1;34m(self, text, dest, src, override)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_translate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, dest, src, override):\n\u001b[1;32m---> 78\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_acquirer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     params \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mbuild_params(query\u001b[38;5;241m=\u001b[39mtext, src\u001b[38;5;241m=\u001b[39msrc, dest\u001b[38;5;241m=\u001b[39mdest,\n\u001b[0;32m     80\u001b[0m                                 token\u001b[38;5;241m=\u001b[39mtoken, override\u001b[38;5;241m=\u001b[39moverride)\n\u001b[0;32m     82\u001b[0m     url \u001b[38;5;241m=\u001b[39m urls\u001b[38;5;241m.\u001b[39mTRANSLATE\u001b[38;5;241m.\u001b[39mformat(host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pick_service_url())\n",
      "File \u001b[1;32mc:\\Users\\gibra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macquire(text)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tk\n",
      "File \u001b[1;32mc:\\Users\\gibra\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRE_TKK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m code \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39mencode()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode-escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "extract_ne('input_text.pdf', 'colored_text.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
